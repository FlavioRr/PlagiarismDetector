{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_java_file(file_path):\n",
    "    \"\"\"Function to read a Java file and return its content as a string.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ast_nodes(code):\n",
    "    \"\"\"Extract AST nodes from Java code.\"\"\"\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(code))\n",
    "        parser = javalang.parser.Parser(tokens)\n",
    "        tree = parser.parse()\n",
    "\n",
    "        ast_nodes = []\n",
    "        for path, node in tree:\n",
    "            if isinstance(node, javalang.tree.Node):\n",
    "                ast_nodes.append(node.__class__.__name__)\n",
    "        return ' '.join(ast_nodes)\n",
    "    except (javalang.parser.JavaSyntaxError, javalang.tokenizer.LexerError) as e:\n",
    "        print(f\"Error al analizar el código Java: {e}\")\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_ast(directory):\n",
    "    \"\"\"Preprocess all Java files in the specified directory and return a DataFrame.\"\"\"\n",
    "    data = []\n",
    "    labels_path = os.path.join(directory, 'versions', 'labels.csv')  # Correct path to labels.csv\n",
    "    labels = pd.read_csv(labels_path)\n",
    "    \n",
    "    # Verificar las columnas del archivo CSV\n",
    "    print(\"Columnas disponibles en labels.csv:\", labels.columns)\n",
    "    \n",
    "    # Asegurarse de que la columna correcta está presente\n",
    "    label_column = 'veredict'  # Cambia esto si el nombre de la columna es diferente\n",
    "    if label_column not in labels.columns:\n",
    "        raise KeyError(f\"La columna '{label_column}' no se encuentra en labels.csv\")\n",
    "    \n",
    "    for index, row in labels.iterrows():\n",
    "        sub1 = row['sub1']\n",
    "        sub2 = row['sub2']\n",
    "        label = row[label_column]  # Use the correct label column\n",
    "        pair_id = f\"{sub1}_{sub2}\"  # Construct pair_id from sub1 and sub2\n",
    "        pair_folder = os.path.join(directory, 'versions', 'version_2', pair_id)\n",
    "        \n",
    "        # Paths to the two Java files in the pair folder\n",
    "        file1_path = os.path.join(pair_folder, f\"{sub1}.java\")\n",
    "        file2_path = os.path.join(pair_folder, f\"{sub2}.java\")\n",
    "        \n",
    "        # Read and extract AST nodes from both files\n",
    "        if os.path.exists(file1_path) and os.path.exists(file2_path):\n",
    "            code1 = read_java_file(file1_path)\n",
    "            code2 = read_java_file(file2_path)\n",
    "            ast_nodes1 = extract_ast_nodes(code1)\n",
    "            ast_nodes2 = extract_ast_nodes(code2)\n",
    "            \n",
    "            if ast_nodes1 and ast_nodes2:  # Only combine if both are non-empty\n",
    "                # Combine the AST nodes from both files\n",
    "                combined_ast_nodes = ast_nodes1 + ' ' + ast_nodes2\n",
    "                data.append([combined_ast_nodes, label])\n",
    "    \n",
    "    return pd.DataFrame(data, columns=['code', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas disponibles en labels.csv: Index(['sub1', 'sub2', 'problem', 'veredict'], dtype='object')\n",
      "Error al analizar el código Java: \n",
      "                                                code  label\n",
      "0  CompilationUnit Import Import ClassDeclaration...      0\n",
      "1  CompilationUnit Import ClassDeclaration Method...      0\n",
      "2  CompilationUnit Import Import ClassDeclaration...      1\n",
      "3  CompilationUnit Import Import Import Import Cl...      0\n",
      "4  CompilationUnit Import Import Import ClassDecl...      0\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "data_directory = r'C:\\Users\\droid\\Documents\\Aplicaciones_Avanzadas\\Proyecto\\PlagiarismDetector\\data\\conplag_version_2'\n",
    "processed_data = preprocess_data_with_ast(data_directory)\n",
    "print(processed_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to numerical features using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust max_features as needed\n",
    "X = tfidf_vectorizer.fit_transform(processed_data['code'])\n",
    "y = processed_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DMatrix, which is a data structure that XGBoost uses\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # for binary classification\n",
    "    'max_depth': 8,  # You can tune this parameter\n",
    "    'eta': 0.3,  # Learning rate\n",
    "    'eval_metric': 'logloss'  # Evaluation metric\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_rounds = 100  # Number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "preds = bst.predict(dtest)\n",
    "predictions = [round(value) for value in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.97%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = sum([1 if pred == label else 0 for pred, label in zip(predictions, y_test)]) / len(y_test)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plagiarismDetector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
